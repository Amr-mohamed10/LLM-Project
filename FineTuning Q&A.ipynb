{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Necessary Libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"context\": \"The Nile River is the lifeblood of Egypt, providing water for agriculture, industry, and domestic use.\",\n",
    "        \"question\": \"What is the importance of the Nile River to Egypt?\",\n",
    "        \"answer\": \"lifeblood of Egypt\"  # Exact match\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Egypt has implemented conservation projects to protect the Nile River's ecosystem and biodiversity.\",\n",
    "        \"question\": \"What are some of Egypt's conservation initiatives?\",\n",
    "        \"answer\": \"conservation projects\" # Exact match\n",
    "    }\n",
    "    # Add more question-answer pairs, ensuring answers are exact substrings of the context.\n",
    "]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            item['question'],\n",
    "            item['context'],\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        # Ensuring the answer is within the context\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        if item['answer'] in item['context']:\n",
    "            start_idx = item['context'].index(item['answer'])\n",
    "            end_idx = start_idx + len(item['answer'])\n",
    "        else:\n",
    "            print(f\"Answer not found in context: {item['answer']}\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'start_positions': torch.tensor(start_idx),\n",
    "            'end_positions': torch.tensor(end_idx)\n",
    "        }\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Device configuration (for GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = QADataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Set up the optimizer and scheduler (using torch.optim.AdamW)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(dataloader) * 3  # Number of epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Fine-tune the model\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Move input data to the correct device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                        start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1} completed\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine-tuned-distilbert')\n",
    "tokenizer.save_pretrained('fine-tuned-distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch nltk pymupdf fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1653d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk\n",
    "\n",
    "# Ensure the stopwords and wordnet are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, model_name='fine-tuned-distilbert', max_len=512):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = DistilBertForQuestionAnswering.from_pretrained(model_name).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def get_best_answer(self, question, context, stride=1):\n",
    "        # Tokenize question and context\n",
    "        question_encodings = self.tokenizer.encode_plus(\n",
    "            question, add_special_tokens=True, return_tensors=\"pt\"\n",
    "        ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        context_tokens = self.tokenizer.tokenize(context)\n",
    "\n",
    "        best_answer = \"\"\n",
    "        highest_score = -float('inf')\n",
    "\n",
    "        # Lemmatization of keywords\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        expanded_keywords = set(lemmatizer.lemmatize(kw) for kw in question.lower().split())\n",
    "\n",
    "        # Iterate over tokenized context with stride\n",
    "        for i in range(0, len(context_tokens), stride):\n",
    "            chunk_start = i\n",
    "            chunk_end = min(i + self.max_len - len(question_encodings['input_ids'][0]) - 3, len(context_tokens))\n",
    "            chunk_tokens = context_tokens[chunk_start:chunk_end]\n",
    "            chunk = self.tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                question,\n",
    "                chunk,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=\"only_second\",\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length'\n",
    "            ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            outputs = self.model(**inputs)\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "            start_indexes = torch.topk(start_logits, k=3).indices[0].tolist()\n",
    "            end_indexes = torch.topk(end_logits, k=3).indices[0].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Check for a valid answer span within the context (not including the question)\n",
    "                    if start_index <= end_index < len(inputs[\"input_ids\"][0]) and start_index >= len(question_encodings[\"input_ids\"][0]):\n",
    "                        answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "                        answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "                        # Answer validation\n",
    "                        if len(answer) < 5 or answer.startswith(\".\") or answer.endswith(\".\") or \"[PAD]\" in answer:\n",
    "                            continue\n",
    "\n",
    "                        # Refined scoring\n",
    "                        logit_score = (start_logits[0, start_index] + end_logits[0, end_index]).item()\n",
    "                        keyword_score = sum(\n",
    "                            max(fuzz.ratio(keyword, answer.lower()), fuzz.partial_ratio(keyword, answer.lower())) \n",
    "                            for keyword in expanded_keywords\n",
    "                        ) / 100  # Normalize to 0-1 range\n",
    "\n",
    "                        score = logit_score + keyword_score\n",
    "\n",
    "                        if score > highest_score:\n",
    "                            highest_score = score\n",
    "\n",
    "                            # Post-processing: Find the first keyword and start the answer from there\n",
    "                            for i in range(len(answer_tokens)):\n",
    "                                if self.tokenizer.decode([answer_tokens[i]]).lower() in expanded_keywords:\n",
    "                                    best_answer = self.tokenizer.decode(answer_tokens[i:], skip_special_tokens=True).strip()\n",
    "                                    break  # Stop after the first keyword is found\n",
    "                            else: \n",
    "                                best_answer = answer\n",
    "\n",
    "        return best_answer\n",
    "\n",
    "# Example usage\n",
    "model = Model('fine-tuned-distilbert')\n",
    "context = \"The Nile River is the heart of Egypt's ecosystem. It has supported human populations and agriculture for millennia. Egypt has undertaken various conservation initiatives.\"\n",
    "question = \"What are the conservation initiatives in Egypt?\"\n",
    "\n",
    "answer = model.get_best_answer(question, context)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e3596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-env)",
   "language": "python",
   "name": "ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
