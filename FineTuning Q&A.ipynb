{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14b979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\amr\\anaconda3\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: torch in c:\\users\\amr\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\amr\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\amr\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\amr\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\amr\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\amr\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\amr\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\amr\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n",
      "Epoch 2 completed\n",
      "Epoch 3 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine-tuned-distilbert\\\\tokenizer_config.json',\n",
       " 'fine-tuned-distilbert\\\\special_tokens_map.json',\n",
       " 'fine-tuned-distilbert\\\\vocab.txt',\n",
       " 'fine-tuned-distilbert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Install Necessary Libraries\n",
    "!pip install transformers torch\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"context\": \"The Nile River is the lifeblood of Egypt, providing water for agriculture, industry, and domestic use.\",\n",
    "        \"question\": \"What is the importance of the Nile River to Egypt?\",\n",
    "        \"answer\": \"lifeblood of Egypt\"  # Exact match\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Egypt has implemented conservation projects to protect the Nile River's ecosystem and biodiversity.\",\n",
    "        \"question\": \"What are some of Egypt's conservation initiatives?\",\n",
    "        \"answer\": \"conservation projects\" # Exact match\n",
    "    }\n",
    "    # Add more question-answer pairs, ensuring answers are exact substrings of the context.\n",
    "]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            item['question'],\n",
    "            item['context'],\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        # Ensuring the answer is within the context\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        if item['answer'] in item['context']:\n",
    "            start_idx = item['context'].index(item['answer'])\n",
    "            end_idx = start_idx + len(item['answer'])\n",
    "        else:\n",
    "            print(f\"Answer not found in context: {item['answer']}\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'start_positions': torch.tensor(start_idx),\n",
    "            'end_positions': torch.tensor(end_idx)\n",
    "        }\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Device configuration (for GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = QADataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Set up the optimizer and scheduler (using torch.optim.AdamW)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(dataloader) * 3  # Number of epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Fine-tune the model\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Move input data to the correct device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                        start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1} completed\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine-tuned-distilbert')\n",
    "tokenizer.save_pretrained('fine-tuned-distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f9a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: torch in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (1.24.7)\n",
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.6 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from pymupdf) (1.24.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\amr\\anaconda3\\envs\\ai-env\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch nltk pymupdf fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1653d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMR\\anaconda3\\envs\\ai-env\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AMR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AMR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the conservation initiatives in Egypt?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk\n",
    "\n",
    "# Ensure the stopwords and wordnet are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, model_name='fine-tuned-distilbert', max_len=512):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = DistilBertForQuestionAnswering.from_pretrained(model_name).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def get_best_answer(self, question, context, stride=1):\n",
    "        # Tokenize question and context\n",
    "        question_encodings = self.tokenizer.encode_plus(\n",
    "            question, add_special_tokens=True, return_tensors=\"pt\"\n",
    "        ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        context_tokens = self.tokenizer.tokenize(context)\n",
    "\n",
    "        best_answer = \"\"\n",
    "        highest_score = -float('inf')\n",
    "\n",
    "        # Lemmatization of keywords\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        expanded_keywords = set(lemmatizer.lemmatize(kw) for kw in question.lower().split())\n",
    "\n",
    "        # Iterate over tokenized context with stride\n",
    "        for i in range(0, len(context_tokens), stride):\n",
    "            chunk_start = i\n",
    "            chunk_end = min(i + self.max_len - len(question_encodings['input_ids'][0]) - 3, len(context_tokens))\n",
    "            chunk_tokens = context_tokens[chunk_start:chunk_end]\n",
    "            chunk = self.tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                question,\n",
    "                chunk,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=\"only_second\",\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length'\n",
    "            ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            outputs = self.model(**inputs)\n",
    "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "            start_indexes = torch.topk(start_logits, k=3).indices[0].tolist()\n",
    "            end_indexes = torch.topk(end_logits, k=3).indices[0].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Check for a valid answer span within the context (not including the question)\n",
    "                    if start_index <= end_index < len(inputs[\"input_ids\"][0]) and start_index >= len(question_encodings[\"input_ids\"][0]):\n",
    "                        answer_tokens = inputs[\"input_ids\"][0][start_index:end_index + 1]\n",
    "                        answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "                        # Answer validation\n",
    "                        if len(answer) < 5 or answer.startswith(\".\") or answer.endswith(\".\") or \"[PAD]\" in answer:\n",
    "                            continue\n",
    "\n",
    "                        # Refined scoring\n",
    "                        logit_score = (start_logits[0, start_index] + end_logits[0, end_index]).item()\n",
    "                        keyword_score = sum(\n",
    "                            max(fuzz.ratio(keyword, answer.lower()), fuzz.partial_ratio(keyword, answer.lower())) \n",
    "                            for keyword in expanded_keywords\n",
    "                        ) / 100  # Normalize to 0-1 range\n",
    "\n",
    "                        score = logit_score + keyword_score\n",
    "\n",
    "                        if score > highest_score:\n",
    "                            highest_score = score\n",
    "\n",
    "                            # Post-processing: Find the first keyword and start the answer from there\n",
    "                            for i in range(len(answer_tokens)):\n",
    "                                if self.tokenizer.decode([answer_tokens[i]]).lower() in expanded_keywords:\n",
    "                                    best_answer = self.tokenizer.decode(answer_tokens[i:], skip_special_tokens=True).strip()\n",
    "                                    break  # Stop after the first keyword is found\n",
    "                            else: \n",
    "                                best_answer = answer\n",
    "\n",
    "        return best_answer\n",
    "\n",
    "# Example usage\n",
    "model = Model('fine-tuned-distilbert')\n",
    "context = \"The Nile River is the heart of Egypt's ecosystem. It has supported human populations and agriculture for millennia. Egypt has undertaken various conservation initiatives.\"\n",
    "question = \"What are the conservation initiatives in Egypt?\"\n",
    "\n",
    "answer = model.get_best_answer(question, context)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e3596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-env)",
   "language": "python",
   "name": "ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
